{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### STEP1: Generate COCO Style Annotation\n",
    "\n",
    "Here we show a basic usage example using DemoDataset in `data/DemoDataset/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/home/qiu/Projects/Self-Correction-Human-Parsing/mhp_extension\n"
    }
   ],
   "source": [
    "cd mhp_extension/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python ./coco_style_annotation_creator/test_human2coco_format.py \\\n",
    "--dataset 'Demo' \\\n",
    "--json_save_dir './data/DemoDataset/msrcnn_finetune_annotations' \\\n",
    "--test_img_dir './data/DemoDataset/global_pic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP2: Generater Instance Prediciton\n",
    "Here we provide a finetuned cascade_mask_rcnn_X_152_32x8d_FPN_IN5k_gn_dconv model on CIHP dataset with human instance mask. Download the pretrained weight in `pretrain_model/`.\n",
    "\n",
    "- [detectron2_maskrcnn_cihp_finetune.pth](https://drive.google.com/file/d/1T797HPC9V1mmw0cDoVOPSF1F_rrTcGPG/view?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/home/qiu/Projects/Self-Correction-Human-Parsing/mhp_extension/detectron2/tools\n"
    }
   ],
   "source": [
    "cd ./detectron2/tools/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(conv1): Conv2d(\n            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n          (conv2_offset): Conv2d(1024, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (conv2): DeformConv(\n            in_channels=1024, out_channels=1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), groups=32, deformable_groups=1, bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (29): DeformBottleneckBlock(\n          (conv1): Conv2d(\n            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n          (conv2_offset): Conv2d(1024, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (conv2): DeformConv(\n            in_channels=1024, out_channels=1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), groups=32, deformable_groups=1, bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (30): DeformBottleneckBlock(\n          (conv1): Conv2d(\n            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n          (conv2_offset): Conv2d(1024, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (conv2): DeformConv(\n            in_channels=1024, out_channels=1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), groups=32, deformable_groups=1, bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (31): DeformBottleneckBlock(\n          (conv1): Conv2d(\n            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n          (conv2_offset): Conv2d(1024, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (conv2): DeformConv(\n            in_channels=1024, out_channels=1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), groups=32, deformable_groups=1, bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (32): DeformBottleneckBlock(\n          (conv1): Conv2d(\n            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n          (conv2_offset): Conv2d(1024, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (conv2): DeformConv(\n            in_channels=1024, out_channels=1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), groups=32, deformable_groups=1, bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (33): DeformBottleneckBlock(\n          (conv1): Conv2d(\n            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n          (conv2_offset): Conv2d(1024, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (conv2): DeformConv(\n            in_channels=1024, out_channels=1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), groups=32, deformable_groups=1, bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (34): DeformBottleneckBlock(\n          (conv1): Conv2d(\n            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n          (conv2_offset): Conv2d(1024, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (conv2): DeformConv(\n            in_channels=1024, out_channels=1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), groups=32, deformable_groups=1, bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (35): DeformBottleneckBlock(\n          (conv1): Conv2d(\n            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n          (conv2_offset): Conv2d(1024, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (conv2): DeformConv(\n            in_channels=1024, out_channels=1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), groups=32, deformable_groups=1, bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n      )\n      (res5): Sequential(\n        (0): DeformBottleneckBlock(\n          (shortcut): Conv2d(\n            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n          (conv2_offset): Conv2d(2048, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n          (conv2): DeformConv(\n            in_channels=2048, out_channels=2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1), groups=32, deformable_groups=1, bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n        (1): DeformBottleneckBlock(\n          (conv1): Conv2d(\n            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n          (conv2_offset): Conv2d(2048, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (conv2): DeformConv(\n            in_channels=2048, out_channels=2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), groups=32, deformable_groups=1, bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n        (2): DeformBottleneckBlock(\n          (conv1): Conv2d(\n            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n          (conv2_offset): Conv2d(2048, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (conv2): DeformConv(\n            in_channels=2048, out_channels=2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), groups=32, deformable_groups=1, bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n      )\n    )\n  )\n  (proposal_generator): RPN(\n    (rpn_head): StandardRPNHead(\n      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (anchor_generator): DefaultAnchorGenerator(\n      (cell_anchors): BufferList()\n    )\n  )\n  (roi_heads): CascadeROIHeads(\n    (box_pooler): ROIPooler(\n      (level_poolers): ModuleList(\n        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n      )\n    )\n    (box_head): ModuleList(\n      (0): FastRCNNConvFCHead(\n        (conv1): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n        )\n        (conv3): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n        )\n        (conv4): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n        )\n        (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n      )\n      (1): FastRCNNConvFCHead(\n        (conv1): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n        )\n        (conv3): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n        )\n        (conv4): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n        )\n        (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n      )\n      (2): FastRCNNConvFCHead(\n        (conv1): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n        )\n        (conv3): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n        )\n        (conv4): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n        )\n        (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n      )\n    )\n    (box_predictor): ModuleList(\n      (0): FastRCNNOutputLayers(\n        (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n      )\n      (1): FastRCNNOutputLayers(\n        (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n      )\n      (2): FastRCNNOutputLayers(\n        (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n      )\n    )\n    (mask_pooler): ROIPooler(\n      (level_poolers): ModuleList(\n        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n      )\n    )\n    (mask_head): MaskRCNNConvUpsampleHead(\n      (mask_fcn1): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n      )\n      (mask_fcn2): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n      )\n      (mask_fcn3): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n      )\n      (mask_fcn4): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n      )\n      (mask_fcn5): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n      )\n      (mask_fcn6): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n      )\n      (mask_fcn7): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n      )\n      (mask_fcn8): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n      )\n      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n)\n\u001b[32m[07/27 15:54:34 fvcore.common.checkpoint]: \u001b[0mLoading checkpoint from /home/qiu/Downloads/models/detectron2/detectron2_maskrcnn_cihp_finetune.pth\n\u001b[32m[07/27 15:54:35 d2.data.datasets.coco]: \u001b[0mLoaded 4 images in COCO format from ../../data/DemoDataset/msrcnn_finetune_annotations/Demo.json\n\u001b[32m[07/27 15:54:35 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n\u001b[36m|  category  | #instances   |\n|:----------:|:-------------|\n|   person   | 0            |\n|            |              |\u001b[0m\n\u001b[32m[07/27 15:54:35 d2.data.common]: \u001b[0mSerializing 4 elements to byte tensors and concatenating them all ...\n\u001b[32m[07/27 15:54:35 d2.data.common]: \u001b[0mSerialized dataset takes 0.00 MiB\n\u001b[32m[07/27 15:54:35 d2.data.dataset_mapper]: \u001b[0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n\u001b[32m[07/27 15:54:35 d2.evaluation.evaluator]: \u001b[0mStart inference on 4 images\n\u001b[32m[07/27 15:54:37 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:00.490257 (0.490257 s / img per device, on 1 devices)\n\u001b[32m[07/27 15:54:37 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:00 (0.487663 s / img per device, on 1 devices)\n\u001b[32m[07/27 15:54:37 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n\u001b[32m[07/27 15:54:37 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ../../data/DemoDataset/detectron2_prediction/inference/coco_instances_results.json\n\u001b[32m[07/27 15:54:37 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions ...\nLoading and preparing results...\nDONE (t=0.00s)\ncreating index...\nindex created!\nRunning per image evaluation...\nEvaluate annotation type *bbox*\nCOCOeval_opt.evaluate() finished in 0.00 seconds.\nAccumulating evaluation results...\nCOCOeval_opt.accumulate() finished in 0.00 seconds.\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n\u001b[32m[07/27 15:54:37 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n|  AP  |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n|:----:|:------:|:------:|:-----:|:-----:|:-----:|\n| nan  |  nan   |  nan   |  nan  |  nan  |  nan  |\n\u001b[32m[07/27 15:54:37 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\nLoading and preparing results...\nDONE (t=0.00s)\ncreating index...\nindex created!\nRunning per image evaluation...\nEvaluate annotation type *segm*\nCOCOeval_opt.evaluate() finished in 0.00 seconds.\nAccumulating evaluation results...\nCOCOeval_opt.accumulate() finished in 0.00 seconds.\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n\u001b[32m[07/27 15:54:37 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n|  AP  |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n|:----:|:------:|:------:|:-----:|:-----:|:-----:|\n| nan  |  nan   |  nan   |  nan  |  nan  |  nan  |\n\u001b[32m[07/27 15:54:37 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n\u001b[32m[07/27 15:54:37 d2.engine.defaults]: \u001b[0mEvaluation results for demo_val in csv format:\n\u001b[32m[07/27 15:54:37 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n\u001b[32m[07/27 15:54:37 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n\u001b[32m[07/27 15:54:37 d2.evaluation.testing]: \u001b[0mcopypaste: nan,nan,nan,nan,nan,nan\n\u001b[32m[07/27 15:54:37 d2.evaluation.testing]: \u001b[0mcopypaste: Task: segm\n\u001b[32m[07/27 15:54:37 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n\u001b[32m[07/27 15:54:37 d2.evaluation.testing]: \u001b[0mcopypaste: nan,nan,nan,nan,nan,nan\n"
    }
   ],
   "source": [
    "!python finetune_net.py \\\n",
    "--num-gpus 1 \\\n",
    "--config-file ../configs/Misc/demo.yaml \\\n",
    "--eval-only MODEL.WEIGHTS /home/qiu/Downloads/models/detectron2/detectron2_maskrcnn_cihp_finetune.pth TEST.AUG.ENABLED False DATALOADER.NUM_WORKERS 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "source": [
    "!python finetune_net.py \\\n",
    "--num-gpus 1 \\\n",
    "--config-file ../configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x_copy.yaml \\\n",
    "--eval-only MODEL.WEIGHTS /home/qiu/Downloads/models/detectron2/detectron2_mask_rcnn_fpn_3x_pre.pkl TEST.AUG.ENABLED False DATALOADER.NUM_WORKERS 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crop the original image by prediction bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/home/qiu/Projects/Self-Correction-Human-Parsing/mhp_extension\n"
    }
   ],
   "source": [
    "cd ../../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!python make_crop_and_mask_w_mask_nms.py \\\n",
    "--img_dir './data/DemoDataset/global_pic' \\ \n",
    "--save_dir './data/DemoDataset' \\\n",
    "--img_list './data/DemoDataset/annotations/Demo.json' \\\n",
    "--det_res './data/DemoDataset/detectron2_prediction/inference/instances_predictions.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00, 13.71it/s]\n"
    }
   ],
   "source": [
    "!python make_crop_and_mask_w_mask_nms.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP3: Predict Local and Global Result\n",
    "Download the pretrained weight in `pretrain_model/`.\n",
    "\n",
    "- [exp_schp_multi_cihp_global.pth](https://drive.google.com/file/d/1s30hj8zeYj0wuTA5Rek-one-v5uT7kX9/view?usp=sharing)\n",
    "- [exp_schp_multi_cihp_local.pth](https://drive.google.com/file/d/1dwDrXHkhAe_nYtnSqi548zrjo5mlSPF0/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/home/qiu/Projects/Self-Correction-Human-Parsing\n"
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTHONPATH=$PYTHONPATH:/home/qiu/Projects/Self-Correction-Human-Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "import os\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "image mean: [0.406, 0.456, 0.485]\nimage std: [0.225, 0.224, 0.229]\ninput space:BGR\nBGR Transformation\nTotoal testing sample numbers: 17\n100%|███████████████████████████████████████████| 17/17 [00:02<00:00,  6.94it/s]\n"
    }
   ],
   "source": [
    "!python mhp_extension/global_local_parsing/global_local_evaluate.py \\\n",
    "--data-dir mhp_extension/data/DemoDataset \\\n",
    "--split-name crop_pic \\\n",
    "--model-restore /home/qiu/Projects/Self-Correction-Human-Parsing/deploy/pascal_abn_checkpoint.pth \\\n",
    "--log-dir mhp_extension/data/DemoDataset \\\n",
    "--save-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "image mean: [0.406, 0.456, 0.485]\nimage std: [0.225, 0.224, 0.229]\ninput space:BGR\nBGR Transformation\nTotoal testing sample numbers: 4\n100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.29it/s]\n"
    }
   ],
   "source": [
    "!python mhp_extension/global_local_parsing/global_local_evaluate.py \\\n",
    "--data-dir mhp_extension/data/DemoDataset \\\n",
    "--split-name global_pic \\\n",
    "--model-restore /home/qiu/Projects/Self-Correction-Human-Parsing/deploy/pascal_abn_checkpoint.pth \\\n",
    "--log-dir mhp_extension/data/DemoDataset \\\n",
    "--save-results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP4: Fusion Prediciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[Parallel(n_jobs=24)]: Using backend LokyBackend with 24 concurrent workers.\n[Parallel(n_jobs=24)]: Done   1 out of   4 | elapsed:    1.6s remaining:    4.9s\n[Parallel(n_jobs=24)]: Done   2 out of   4 | elapsed:    5.0s remaining:    5.0s\n[Parallel(n_jobs=24)]: Done   4 out of   4 | elapsed:   12.1s remaining:    0.0s\n[Parallel(n_jobs=24)]: Done   4 out of   4 | elapsed:   12.1s finished\n"
    }
   ],
   "source": [
    "!python mhp_extension/logits_fusion.py \\\n",
    "--test_json_path ./mhp_extension/data/DemoDataset/crop.json \\\n",
    "--global_output_dir ./mhp_extension/data/DemoDataset/global_pic_parsing \\\n",
    "--gt_output_dir ./mhp_extension/data/DemoDataset/crop_pic_parsing \\\n",
    "--mask_output_dir ./mhp_extension/data/DemoDataset/crop_mask \\\n",
    "--save_dir ./mhp_extension/data/DemoDataset/mhp_fusion_parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from PIL import Image\n",
    "Image.open('./mhp_extension/demo/demo.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image.open('./mhp_extension/demo/demo_instance_human_mask.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image.open('./mhp_extension/demo/demo_global_human_parsing.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image.open('./mhp_extension/demo/demo_multiple_human_parsing.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('de2': conda)",
   "language": "python",
   "name": "python_defaultSpec_1595580497655"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "## COCO style annotation transfer"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}